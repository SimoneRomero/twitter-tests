{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import time\n",
    "import schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define keys\n",
    "consumer_key = \"YOUR_CONSUMER_KEY\"\n",
    "consumer_secret = \"YOUR_CONSUMER_SECRET\"\n",
    "access_token = \"YOUT_ACCESS_TOKEN\"\n",
    "access_token_secret = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
    "\n",
    "#connection\n",
    "#wait_on_rate_limit, it manages the amount of requests executed \n",
    "#according to the rate limit defined by the Twitter API\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file to save the tweets\n",
    "tweets_saved = \"tweets_crawled.csv\"\n",
    "\n",
    "#columns to describe what is saved\n",
    "COLS = [\"id\", \"created_at\", \"text\", \"name\", \"screen_name\", \"user_location\", \"tweet_place_country\", \"tweet_place_city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of the keywords or queries\n",
    "#the -filter:retweets is to avoid retweets\n",
    "search_words = \"(viajar AND nordeste) -filter:retweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters: path of the file to save the tweets\n",
    "\n",
    "This method aims at looking for tweets given a keyword or a query. \n",
    "In this case, we want only tweets in portuguese and don't want retweets. \n",
    "The tweets components that are important to the analysis are going to be \n",
    "saved in the file: id, creation data, text, name, username, the location\n",
    "informed by the user, and the location at moment the user posted the \n",
    "message (when exist).\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def tweets_crawler(file):\n",
    "    #if the file exists, then read the existing data\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file, header = 0)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns = COLS)\n",
    "    \n",
    "    \n",
    "    #do the search on Twitter, using the keywords defined, only portuguese.\n",
    "    tweets = tw.Cursor(api.search, q = search_words, lang='pt').items(10)            \n",
    "    \n",
    "    #get the necessary data from tweets\n",
    "    for tweet in tweets:\n",
    "        #new_entry append\n",
    "        new_entry = []\n",
    "        new_entry = [tweet.id, tweet.created_at, tweet.text, tweet.user.name, \n",
    "                             tweet.user.screen_name, tweet.user.location]\n",
    "        \n",
    "        #in the case the 'place' has not been informed\n",
    "        if(tweet.place):\n",
    "            new_entry.append(tweet.place.country)\n",
    "            new_entry.append(tweet.place.name)\n",
    "        else:\n",
    "            tweet_place_country = None      \n",
    "            tweet_place_city  = None\n",
    "            new_entry.append(tweet_place_country)\n",
    "            new_entry.append(tweet_place_city)\n",
    "                \n",
    "        single_tweet_df = pd.DataFrame([new_entry], columns = COLS)\n",
    "        df = df.append(single_tweet_df, ignore_index = True)\n",
    "    \n",
    "    #save the tweets\n",
    "    csvFile = open(file, 'a')            \n",
    "    df.to_csv(csvFile, mode = 'a', columns = COLS, index = False, encoding=\"utf-8\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job():\n",
    "    tweets_crawler(tweets_saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #execute every friday at 8p.m.\n",
    "    schedule.every().friday.at(\"20:00\").do(job)\n",
    "    \n",
    "    #to keep the container running\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
